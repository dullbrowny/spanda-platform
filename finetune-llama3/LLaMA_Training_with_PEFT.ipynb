{"cells":[{"cell_type":"markdown","metadata":{},"source":["# LLaMA Training with PEFT"]},{"cell_type":"markdown","metadata":{},"source":["## Step 1: Data Preparation\n","\n","In this step, we load the dataset, format the instructions and responses, and filter the examples based on token length."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"Invalid token passed!","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[4], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m api_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myour_huggingface_api_key\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Log in to Hugging Face and save the token to git credentials helper\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mlogin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_to_git_credential\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Initialize tokenizer\u001b[39;00m\n\u001b[0;32m     16\u001b[0m base_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3-8B\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","File \u001b[1;32mc:\\Users\\risha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\_login.py:111\u001b[0m, in \u001b[0;36mlogin\u001b[1;34m(token, add_to_git_credential, new_session, write_permission)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m add_to_git_credential:\n\u001b[0;32m    105\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe token has not been saved to the git credentials helper. Pass \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`add_to_git_credential=True` in this function directly or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`--add-to-git-credential` if using via `huggingface-cli` if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou want to set the git credential as well.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m         )\n\u001b[1;32m--> 111\u001b[0m     \u001b[43m_login\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_to_git_credential\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_to_git_credential\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_permission\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_permission\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_notebook():\n\u001b[0;32m    113\u001b[0m     notebook_login(new_session\u001b[38;5;241m=\u001b[39mnew_session, write_permission\u001b[38;5;241m=\u001b[39mwrite_permission)\n","File \u001b[1;32mc:\\Users\\risha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\_login.py:307\u001b[0m, in \u001b[0;36m_login\u001b[1;34m(token, add_to_git_credential, write_permission)\u001b[0m\n\u001b[0;32m    305\u001b[0m permission \u001b[38;5;241m=\u001b[39m get_token_permission(token)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m permission \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 307\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid token passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m write_permission \u001b[38;5;129;01mand\u001b[39;00m permission \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    310\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToken is valid but is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread-only\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwrite\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m token is required.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease provide a new token with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m correct permission.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    312\u001b[0m     )\n","\u001b[1;31mValueError\u001b[0m: Invalid token passed!"]}],"source":["# Import necessary libraries\n","import pandas as pd\n","from datasets import Dataset\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from transformers import AutoTokenizer\n","from huggingface_hub import login\n","\n","# Replace 'your_huggingface_api_key' with your actual Hugging Face API key\n","api_key = 'your_huggingface_api_key'\n","\n","# Log in to Hugging Face and save the token to git credentials helper\n","login(token=api_key, add_to_git_credential=True)\n","\n","# Initialize tokenizer\n","base_model = \"meta-llama/Meta-Llama-3-8B\"\n","tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\"\n","\n","def prepare_data(csv_path):\n","    df = pd.read_csv(csv_path)\n","\n","    df[\"formatted_instruction\"] = df.apply(lambda x: f\"### Instruction:\\n{x['Instructions']}\\n\\n### Response:\\n{x['Responses']}\", axis=1)\n","    df[\"formatted_instruction_tok_len\"] = df[\"formatted_instruction\"].apply(lambda x: len(tokenizer(x)[\"input_ids\"]))\n","\n","    sns.boxplot(x=df[\"formatted_instruction_tok_len\"])\n","    plt.xlabel(\"formatted_instruction_tok_len\")\n","    plt.title(\"Token Length\")\n","    plt.show()\n","\n","    df = df[df[\"formatted_instruction_tok_len\"] <= 128]\n","    dataset = Dataset.from_pandas(df)\n","    return dataset\n","\n","# Specify the path to your dataset\n","csv_path = 'path_to_your_dataset.csv'\n","# Prepare the dataset\n","dataset = prepare_data(csv_path)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Step 2: Model and Tokenizer Initialization\n","\n","Here, we initialize the model and tokenizer. We use 4-bit quantization for efficient training."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import necessary libraries\n","import torch\n","from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n","\n","# Set compute dtype\n","compute_dtype = torch.float16\n","\n","# Configure quantization\n","quant_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=False,\n",")\n","\n","# Load the model with quantization configuration\n","model = AutoModelForCausalLM.from_pretrained(\n","    base_model,\n","    quantization_config=quant_config,\n","    device_map={\"\": 0}\n",")\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1"]},{"cell_type":"markdown","metadata":{},"source":["## Step 3: Calculate Model Size\n","\n","We define a function to calculate the model size in MB."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to calculate model size\n","def calculate_model_size(model):\n","    total_size = 0\n","    for param in model.parameters():\n","        param_size = param.numel() * param.element_size()\n","        total_size += param_size\n","    total_size_in_mb = total_size / (1024**2)\n","    return total_size_in_mb\n","\n","# Calculate and print model size\n","model_size_mb = calculate_model_size(model)\n","print(f\"Model size: {model_size_mb:.2f} MB\")"]},{"cell_type":"markdown","metadata":{},"source":["## Step 4: PEFT Configuration\n","\n","Set up PEFT parameters for training."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import necessary libraries\n","from peft import LoraConfig\n","\n","# Configure PEFT parameters\n","peft_params = LoraConfig(\n","    lora_alpha=16,\n","    lora_dropout=0.1,\n","    r=64,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\"\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Step 5: Training Arguments\n","\n","Set the training arguments."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import necessary libraries\n","from transformers import TrainingArguments\n","\n","# Set training arguments\n","training_params = TrainingArguments(\n","    output_dir=\"./\",\n","    num_train_epochs=1,\n","    per_device_train_batch_size=2,\n","    gradient_accumulation_steps=1,\n","    optim=\"paged_adamw_32bit\",\n","    save_steps=50,\n","    logging_steps=25,\n","    learning_rate=2e-4,\n","    weight_decay=0.001,\n","    fp16=False,\n","    bf16=False,\n","    max_grad_norm=0.3,\n","    max_steps=-1,\n","    warmup_ratio=0.03,\n","    group_by_length=True,\n","    lr_scheduler_type=\"constant\",\n","    report_to=\"tensorboard\"\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Step 6: SFTTrainer Setup and Training\n","\n","Initialize `SFTTrainer` and train the model."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import necessary libraries\n","from trl import SFTTrainer\n","import time\n","\n","# Initialize SFTTrainer\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_params,\n","    dataset_text_field=\"formatted_instruction\",\n","    max_seq_length=128,\n","    tokenizer=tokenizer,\n","    args=training_params,\n","    packing=False,\n",")\n","\n","# Train the model\n","start = time.time()\n","output = trainer.train()\n","print(\"Time taken: \", time.time() - start)"]},{"cell_type":"markdown","metadata":{},"source":["## Step 7: Monitor Training with Tensorboard\n","\n","Start Tensorboard for monitoring the training process."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Start Tensorboard for monitoring\n","from tensorboard import notebook\n","\n","log_dir = \"runs\"\n","notebook.start(\"--logdir {} --port 4000\".format(log_dir))"]},{"cell_type":"markdown","metadata":{},"source":["## Step 8: Generate Text Using the Trained Model\n","\n","Use the trained model to generate text based on a given prompt."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import necessary libraries\n","from transformers import pipeline\n","\n","# Initialize text generation pipeline\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=100)\n","\n","# Define prompt\n","prompt = \"### Instruction:\\nPlease generate 5 variants of the question: 'What is the capital of France?'\\n\\n### Response\\n:\"\n","\n","# Generate text\n","gen_text = pipe(prompt)\n","print(gen_text[0]['generated_text'][len(prompt):])"]},{"cell_type":"markdown","metadata":{},"source":["### Implementation Notes\n","\n","- Ensure you have the required libraries installed (`transformers`, `datasets`, `peft`, `trl`, `seaborn`, `matplotlib`, `tensorboard`).\n","- Adjust the file path for your dataset in `csv_path`.\n","- The token length filter and other parameters can be adjusted based on your specific use case and dataset.\n","- Monitor the training process using Tensorboard to check for a gradual decline in training loss."]},{"cell_type":"markdown","metadata":{},"source":["### Inserting Your Hugging Face API Key\n","\n","To access the gated model, you need to use your Hugging Face API key. You can insert your API key by running the following code:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Insert your Hugging Face API key\n","token = \"your_huggingface_api_key\"\n","from huggingface_hub import notebook_login\n","notebook_login(token=token)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":4}
